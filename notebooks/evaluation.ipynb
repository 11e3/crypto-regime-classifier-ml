{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crypto Regime Classifier - Model Evaluation\n",
    "\n",
    "This notebook evaluates trained regime classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "\n",
    "from src.features import FeatureExtractor\n",
    "from src.labeling import RegimeLabeler, RegimeType\n",
    "from src.models import RegimeClassifier, EnsembleClassifier\n",
    "from src.utils.data import load_ohlcv, prepare_training_data\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and Prepare Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update path to your data file\n",
    "DATA_PATH = \"../data/BTC.csv\"\n",
    "\n",
    "try:\n",
    "    df = load_ohlcv(DATA_PATH)\n",
    "    print(f\"Loaded {len(df)} rows\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Data file not found at {DATA_PATH}\")\n",
    "    # Create sample data for demonstration\n",
    "    np.random.seed(42)\n",
    "    dates = pd.date_range('2020-01-01', periods=1000, freq='D')\n",
    "    price = 10000 * np.exp(np.cumsum(np.random.randn(1000) * 0.02))\n",
    "    df = pd.DataFrame({\n",
    "        'open': price * (1 + np.random.randn(1000) * 0.01),\n",
    "        'high': price * (1 + np.abs(np.random.randn(1000)) * 0.02),\n",
    "        'low': price * (1 - np.abs(np.random.randn(1000)) * 0.02),\n",
    "        'close': price,\n",
    "        'volume': np.random.lognormal(20, 1, 1000)\n",
    "    }, index=dates)\n",
    "    print(\"Using synthetic data for demonstration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize components\n",
    "extractor = FeatureExtractor()\n",
    "labeler = RegimeLabeler(trend_threshold=0.02, vol_percentile=80)\n",
    "\n",
    "# Prepare data\n",
    "X_train, X_test, y_train, y_test = prepare_training_data(\n",
    "    df=df,\n",
    "    feature_extractor=extractor,\n",
    "    labeler=labeler,\n",
    "    test_size=0.2\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train and Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple models\n",
    "models = {\n",
    "    'Random Forest': RegimeClassifier(model_type='random_forest'),\n",
    "    'Gradient Boosting': RegimeClassifier(model_type='gradient_boosting'),\n",
    "    'Logistic Regression': RegimeClassifier(model_type='logistic'),\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training {name}\")\n",
    "    print('='*50)\n",
    "    \n",
    "    model.fit(X_train, y_train, verbose=False)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    \n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'accuracy': acc,\n",
    "        'f1_macro': f1,\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"F1 (macro): {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models\n",
    "comparison_df = pd.DataFrame({\n",
    "    name: {'Accuracy': r['accuracy'], 'F1 (macro)': r['f1_macro']}\n",
    "    for name, r in results.items()\n",
    "}).T\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "print(comparison_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, comparison_df['Accuracy'], width, label='Accuracy')\n",
    "bars2 = ax.bar(x + width/2, comparison_df['F1 (macro)'], width, label='F1 (macro)')\n",
    "\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Model Performance Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparison_df.index)\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars1 + bars2:\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(f'{height:.3f}',\n",
    "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                xytext=(0, 3),\n",
    "                textcoords=\"offset points\",\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Confusion Matrix Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model confusion matrix\n",
    "best_model_name = max(results, key=lambda x: results[x]['f1_macro'])\n",
    "best_result = results[best_model_name]\n",
    "\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, best_result['predictions']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix heatmap\n",
    "cm = confusion_matrix(y_test, best_result['predictions'])\n",
    "labels = sorted(y_test.unique())\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Raw counts\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=labels, yticklabels=labels, ax=axes[0])\n",
    "axes[0].set_title(f'{best_model_name} - Confusion Matrix (Counts)')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "\n",
    "# Normalized\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "            xticklabels=labels, yticklabels=labels, ax=axes[1])\n",
    "axes[1].set_title(f'{best_model_name} - Confusion Matrix (Normalized)')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for Random Forest\n",
    "rf_model = results['Random Forest']['model']\n",
    "importance = rf_model.get_feature_importance()\n",
    "\n",
    "# Plot top 20 features\n",
    "top_n = 20\n",
    "top_features = importance.head(top_n)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "top_features.plot(kind='barh', ax=ax)\n",
    "ax.set_xlabel('Importance')\n",
    "ax.set_title(f'Top {top_n} Feature Importance (Random Forest)')\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Regime Prediction Stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check regime persistence\n",
    "predictions = best_result['predictions']\n",
    "\n",
    "# Calculate regime switches\n",
    "switches = (predictions != predictions.shift(1)).sum()\n",
    "total_days = len(predictions)\n",
    "switch_rate = switches / total_days\n",
    "\n",
    "print(f\"Regime switches: {switches}\")\n",
    "print(f\"Total days: {total_days}\")\n",
    "print(f\"Switch rate: {switch_rate:.2%}\")\n",
    "print(f\"Average regime duration: {1/switch_rate:.1f} days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs actual\n",
    "regime_colors = {\n",
    "    'BULL_TREND': 'green',\n",
    "    'BEAR_TREND': 'red',\n",
    "    'SIDEWAYS': 'gray',\n",
    "    'HIGH_VOL': 'orange'\n",
    "}\n",
    "\n",
    "# Get test period price data\n",
    "test_prices = df.loc[y_test.index, 'close']\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "# Price\n",
    "axes[0].plot(test_prices.index, test_prices.values, color='black')\n",
    "axes[0].set_ylabel('Price')\n",
    "axes[0].set_title('Test Period Price')\n",
    "\n",
    "# Actual regimes\n",
    "for regime in RegimeType:\n",
    "    mask = y_test == regime.value\n",
    "    if mask.any():\n",
    "        axes[1].fill_between(\n",
    "            y_test.index, 0, 1,\n",
    "            where=mask,\n",
    "            alpha=0.7,\n",
    "            color=regime_colors[regime.value],\n",
    "            label=regime.value\n",
    "        )\n",
    "axes[1].set_ylabel('Actual')\n",
    "axes[1].legend(loc='upper right', ncol=4)\n",
    "\n",
    "# Predicted regimes\n",
    "for regime in RegimeType:\n",
    "    mask = predictions == regime.value\n",
    "    if mask.any():\n",
    "        axes[2].fill_between(\n",
    "            predictions.index, 0, 1,\n",
    "            where=mask,\n",
    "            alpha=0.7,\n",
    "            color=regime_colors[regime.value],\n",
    "            label=regime.value\n",
    "        )\n",
    "axes[2].set_ylabel('Predicted')\n",
    "axes[2].legend(loc='upper right', ncol=4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ensemble\n",
    "ensemble = EnsembleClassifier(\n",
    "    model_types=['random_forest', 'gradient_boosting'],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "ensemble.fit(X_train, y_train, verbose=False)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_ensemble = ensemble.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred_ensemble)\n",
    "f1 = f1_score(y_test, y_pred_ensemble, average='macro')\n",
    "\n",
    "print(f\"Ensemble Performance:\")\n",
    "print(f\"  Accuracy: {acc:.4f}\")\n",
    "print(f\"  F1 (macro): {f1:.4f}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_ensemble))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model agreement\n",
    "agreements = ensemble.get_model_agreements(X_test)\n",
    "\n",
    "# Calculate agreement rate\n",
    "agreement_rate = (agreements.iloc[:, 0] == agreements.iloc[:, 1]).mean()\n",
    "print(f\"Model agreement rate: {agreement_rate:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best performing model\n",
    "import os\n",
    "\n",
    "output_dir = '../models/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "best_model = results[best_model_name]['model']\n",
    "model_path = f'{output_dir}regime_classifier_best.pkl'\n",
    "\n",
    "best_model.save(model_path)\n",
    "print(f\"Saved best model ({best_model_name}) to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nData: {len(df)} total samples\")\n",
    "print(f\"  Training: {len(X_train)}\")\n",
    "print(f\"  Test: {len(X_test)}\")\n",
    "print(f\"\\nBest Model: {best_model_name}\")\n",
    "print(f\"  Accuracy: {results[best_model_name]['accuracy']:.4f}\")\n",
    "print(f\"  F1 (macro): {results[best_model_name]['f1_macro']:.4f}\")\n",
    "print(f\"\\nEnsemble Performance:\")\n",
    "print(f\"  Accuracy: {acc:.4f}\")\n",
    "print(f\"  F1 (macro): {f1:.4f}\")\n",
    "print(f\"\\nTarget Metrics:\")\n",
    "print(f\"  Accuracy > 60%: {'PASS' if results[best_model_name]['accuracy'] > 0.6 else 'FAIL'}\")\n",
    "print(f\"  F1 > 0.55: {'PASS' if results[best_model_name]['f1_macro'] > 0.55 else 'FAIL'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
